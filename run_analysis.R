# "run_analysis.R" creates clean & tidy datasets out of the data collected
# from the accelerometers from the Samsung Galaxy S smartphones through
# the following steps:

# 1. Merges the training and the test sets to create one data set
# 2. Extracts only the measurements on the mean and standard deviation for 
# each measurement
# 3. Uses descriptive activity names to name the activities in the data set
# 4. Appropriately labels the data set with descriptive variable names
# 5. Creates a second, independent tidy data set with the average of each 
# variable for each activity and each subject


# Loading raw data into R, assuming that all raw data files are located 
# in the working directory

test_set <- read.table("./X_test.txt")
test_activity <- read.table("./y_test.txt")
test_volunteer <- read.table("./subject_test.txt")

train_set <- read.table("./X_train.txt")
train_activity <- read.table("./y_train.txt")
train_volunteer <- read.table("./subject_train.txt")


# Inspecting data & Its dimensions

dim(test_set)
head(test_set)
dim(test_activity)
dim(test_volunteer)

dim(train_set)
head(train_set)
dim(train_activity)
dim(train_volunteer)

# Combining data into one complete dataset with all variables

test_data <- cbind(test_volunteer, test_activity, test_set)
train_data <- cbind (train_volunteer, train_activity, train_set)

complete_dataset <- rbind (test_data, train_data)

# Adding labels to variables using the features vector

Features <- read.table("./features.txt")
dim(Features)
head(Features)
Features_name <- as.character(Features[,2])
names(complete_dataset) <- c("Volunteer_ID", "Activity", Features_name)

# Extracts "only" the measurements on the mean and standard deviation 
# for each measurement (i.e. selecting features containing mean or std)

feature_mean <- grep("mean",names(complete_dataset))
feature_std <- grep("std",names(complete_dataset))

needed_data <- complete_dataset[,sort(c(1,2,feature_mean,feature_std))]

        # Ignoring "meanFreq" columns generated with the "feature_mean" 
        # grep function above

meanFreq <- grep("meanFreq",names(needed_data))
needed_data <- subset(needed_data, select=-meanFreq)


# Use descriptive activity names to name the activities in the data set

activity_labels <- read.table("./activity_labels.txt", 
                              stringsAsFactors = FALSE)
merged <- merge(needed_data, activity_labels, by.x="Activity", by.y="V1"
                , all=TRUE)
merged[,1] <- merged[,ncol(merged)]

        # removing unnecessary column generated by the "merge" function
merged <- merged[,1:(ncol(merged)-1)]


# Arranging the dataset according to Volunteer ID & Activity type plus
# writing it to (.txt) file

library(plyr)
clean_ordered_data <- arrange(merged, Volunteer_ID, Activity)
write.table(clean_ordered_data, file="./clean_ordered_data.txt", 
            row.names = FALSE)


# Creating a 2nd , independent tidy data set with the average of each 
# variable for each activity and each subject

        # 1. Creating a unique (Volunteer-Activity Identifier) 
        # for each variable

clean_ordered_data$Unique_Volunteer_Activity_ID <- 
        do.call(paste, c(clean_ordered_data[c("Volunteer_ID", "Activity")], 
                         sep = ""))

        # 2. Calculating average for each variable per each Unique ID

library(reshape2)
melt_data <- melt(clean_ordered_data, id="Unique_Volunteer_Activity_ID", 
                  measure.vars= names(clean_ordered_data)[3:68])
average_dataset <- dcast (melt_data,Unique_Volunteer_Activity_ID 
                          ~ variable,mean)

        # 3. Changing variables' names to reflect the average
names(average_dataset)[2:67] <- paste("Average - ", 
                        names(average_dataset)[2:67], sep="")

write.table(average_dataset, file="./average_ordered_data.txt", 
            row.names = FALSE)


##########################################################################